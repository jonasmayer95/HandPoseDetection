\chapter{Implementation}\label{chapter:implementation}

This chapter will give some details about the implementation, the implementation process, the problems that occurred an how they were solved.

\section{Basic Setup}

\subsection{Unity}

For the implementation of the hand tracking, the metric computation and the user study, Unity 3D 5.3 was chosen as a programming environment. Technically Unity 3D is a powerful game engine, that can be used for 3d or 2d applications and games. It allows users to easily create scenes with different objects whose behavior can be specified with \texttt{C\#} scripts and are rendered automatically. Due to its complex input manager, it allows the user to receive input from a multitude of input devices and supports most unconventional interaction devices like HMDs and also the Leap Motion Controller. In addition a built-in UI-Manager allows for quick prototyping of user interfaces. 

\subsection{Leap Motion Controller}

Similar to Nikolas Schneider's setup, a Leap Motion Controller was used for the actual hand tracking. The Leap Motion Controller, or simply Leap is an optical hand tracker using three IR-LEDs and two IR-Cameras to record the users hand. By providing an SDK and a Unity package, integration into Unity 3D works seamlessly. With the new Leap Orion SDK, the Leap allows for accurate and robust finger tracking in different lighting conditions and usage contexts. The Leap was chosen due to its easy integration combined with the solid tracking performance, that is unrivaled for hands-free hand trackers.  
Instead of attaching it to the user's arm with a metal construction, potentially causing discomfort to the user, it was simply placed on a desk. 

\subsection{Further Project Configuration}

As using the AR-Rift did not seam to benefit the project substantially, it allowed to set up a clean 3D Unity project. The only imported assets were the Leap Motion Unity core assets which provide useful tools for interacting with the recorded hands. All work was based on the Leap sample scene with two basic hand objects. 

\section{Hand Model}

In order to make different computations with hands, considerations have to be made about how to represent a hand posture in a virtual environment. In the field of free hand computer interaction, two different hand models are commonly used: angle-based and point-based hand models.
%add figure
The point-based hand model describes a hand posture as a set of 6 6DOF (position and orientation) points. 5 of these represent the finger tips, one represents the palm. In the angle-based hand model, a hand posture is described by the hand's joint angles. Depending on the literature, the joints in the hand have a total of 22 \cite{su1994logical} or 23 DOFs \cite{laviola1999survey}, differentiating in the DOFs of the \textit{trapeziometacarpal} joint (TMC, Figure \ref{fig:handAnatomyTotal}).
Even though the two models are almost interchangeable, the angle-based model was chosen, as it makes most computations of metric components trivial. A common simplification for angle-based hand models is to neglect the 2 DOFs, given by the \textit{metacarpocarpal} (MCC, Figure \ref{fig:handAnatomyTotal}) of the forth and fifth digit. This is done for example by the Leap, because the MCCs hardly move and are therefore are not noticeable for most applications. 
In conclusion, the implementation was based on a 21DOF angle based hand model, as it can be seen in \textcolor[rgb]{1,0,0}{Figure ????}.

The goal for the unity implementation was to create a universally usable and flexible class structure for hand postures, thus the \texttt{AngleBasedHandModel} class was created. The class has a reference to a thumb object of class \texttt{AngleBasedThumbModel} and four enumerated fingers of class \texttt{AngleBasedFingerModel}. In addition the hands position and rotation are stored for debugging. The class also provides a function to calculate the mathematical Euclidean distance to another hand posture, a function to interpolate between two hand postures, a toString function and multiple functions for loading and storing hand postures as CSV. 

The \texttt{AngleBasedFingerModel} (and \texttt{AngleBasedThumbModel}) stores their 4 (and 5) DOFs as float angles. To ease computation the rotation of the MCP (and TMC) is redundantly stored as quaternion. Both classes also implement the same functions as \texttt{AngleBasedHandModel} and \texttt{AngleBasedFingerModel} additionally provides a function for computing the total flexion of a finger (\textcolor[rgb]{1,0,0}{Figure ???}). The three classes are fully serializable to facilitate simple saving and loading of hand postures.
%TODO: add figure?
In order to obtain the hand posture data from the the Leap in realtime, a \texttt{HandObserver} script was attached to each Leap hand objects. The \texttt{HandObserver} extracted the hand posture data from the Leap and updated an \texttt{AngleBasedHandModel} object while taking the handedness into account.

\section{Hand Posture Classification}

In order to make sure, that the participant would hold the hand posture during the user study, some kind of hand posture classification system had to be implemented. While there are multiple different methods, that can be used for hand posture classification, the k-nearest neighbors (k-NN) algorithm has proven to be both suitable for hand posture classification and comparatively easy to implement. 

The k-NN algorithm is a simple machine learning algorithm that can be applied to a multitude of problems with an n-dimensional feature vector. The basic idea is to test the membership of an object using training data. The membership of the vector is determined by the majority of it's k nearest neighbors. 

In the case of hand posture classification, we have a 21-dimensional feature vector according to our hand model. The implementation here was based on Nikolas Schneider's implementation, but was refactored to be more versatile in use. Basically the k-NN implementation has two main objectives:

\begin{enumerate}
	\item Data Collection
	\item Actual Posture Classification
\end{enumerate}

For \textbf{data collection} a set of training data, consisting of feature vectors with their classification has to be obtained. This was implemented by the \texttt{PostureDataHandler} class. The \texttt{PostureDataHandler} takes care of the training data, implemented as a list of \texttt{TrainingUnit} objects and provides functions to add and delete \texttt{TrainingUnit}s. A \texttt{TrainingUnit} consists of a \texttt{AngleBasedHandModel} object that is classified with a \texttt{Posture} enumerator. To achieve data persistence, the \texttt{PostureDataHandler} saves the data to the hard drive after every modification and automatically loads it when instantiated.

A TrainingManager scene was created that allows the user to manipulate the training data using a GUI. The user can inspect \texttt{TrainingUnit}s, that are visualized with a \texttt{OutputHand} built from Unity primitives. The \texttt{TrainingUnit}s are grouped by \texttt{Posture} and can be deleted individually or as group. In order to add \texttt{TrainingUnit}s the user has to choose the desired \texttt{Posture}, form the hand posture over the Leap and press a button. 

The \textbf{actual posture classification} classifies a hand posture based on the k nearest neighbors. For that, the distances of the hand posture to all training units has to be computed and the k closest have to be selected. In this case, k is set as the square root of the total training data size.

In the implementation this is handled by the \texttt{ThreadedKNN} class. As the name suggest, the classification is done in parallel to the main program using threads. Each frame the \texttt{ThreadedKNN} starts a new thread which receives the \texttt{AngleBasedHandModel} that needs to be classified. The thread receives a list of \texttt{PoseCompareObject}s from the \texttt{PostureDataHandler} containing the Euclidian distance of every \texttt{TrainingUnit} to the \texttt{AngleBasedHandModel} and its classifying \texttt{Posture}. The thread then sorts the list by distance and counts the occurrences of the different \texttt{Posture}s among the k nearest \texttt{PoseCompareObject}s. The \texttt{AngleBasedHandModel} is then classified as the most frequent \texttt{Posture} and the result is written back to the \texttt{ThreadedKNN} object.

In the context it was used, the k-NN implementation could differentiate between 10 postures with 50 recorded training samples each. The hand posture classification was performed on two hand robustly in real time without noticeable performance impact or lag. 


\section{Hand Posture Metrics}\label{chapter:handosturemetric}

Now that we have a representation for hand postures, the metric can be implemented. For that a \texttt{Comfort} and a \texttt{Discomfort} class were created. Both contain functions for computing the total metric value, the metric components for the whole hand and for the single fingers as well as functions for outputting the finger values to a .CSV file. 

\subsection{Comfort}

The only component, that is computed in \texttt{Comfort} is the distance to the RRP. Theoretically every DOF in the hand has an own RRP, that can be determined through experiments as done by Apostolico et al. \cite{apostolico2014postural}. However this turns out to be a costly and lengthy process, that had exceeded the focus of this thesis. Therefore some simplifications had to be made:

\begin{enumerate}
	\item The RRP would not be determined for every RRP separately but the whole hand simultaneously.
	\item The RRP would not be defined as a continuous range but as a discrete set of samples.
\end{enumerate}

In conclusion of this, the RRP would be defined as a set of relaxed hand postures. To implement this, we simply used the 50 samples of the "idle" \texttt{Posture} stored in the \texttt{PostureDataHandler} to define our RRP. A correct implementation of the distance to the RRP would have been to compute the 21-dimensional bounding volume of the RRP, test a hand posture for collision with the volume and calculate the minimum distance to the volume. Again as a simplification, only the minimum Euclidean distance to any sample of the RRP set was computed. Strictly seen, a hand posture within the RRP could have an RRP value \begin{math}\not= 0\end{math}, 
but early tests showed the error to be negligibly small.
The result were two functions, one that calculated the minimum distance to the RRP for the whole hand, one that calculated it for every single finger individually.

\subsection{Discomfort}

In \texttt{Discomfort} the three discomfort components were computed: inter finger angles, hyperextension and abduction.

For the whole-hand computation of inter finger angles, initially only the absolute flexing differences between the fingers were added up. However, it showed early that the anatomical differences between the individual fingers were so severely influencing the inter finger angle discomfort, that they already had to be compensated in the naive metric. Most of all, the ring finger showed to be incapable of having large flexion differences to its adjacent neighbors. However this only stood out when the ring finger had to stick out between its neighbors, not when it was between them. Therefore a ring finger bonus was added, that was computed as follows: 

\texttt{Mathf.Abs((fingers[middle].getTotalFlexion() - fingers[ring].getTotalFlexion() )- (fingers[ring].getTotalFlexion() - fingers[pinky].getTotalFlexion()));}

This way, the ring bonus only occurred, when the ring finger stuck out between it's neighbors. The ring bonus was multiplied with a weighting coefficient, that was estimated to 1.3.

For computing the single-finger values, index and pinky received the angle differences to their only neighbors while the middle and ring finger values were computed similarly to the ring bonus. 

The computation of the hyperextension was more straight forward. Hyper-extended fingers due to the nature of our hand model have negative extension angles in the MCP. Consequently the single finger hyper extension values are the absolute extension angle of the MCP if the finger is hyper-extended otherwise 0. 

Due to the angle-based hand model, computing the single finger abduction component, comes down calculating the absolute of the fingers MCP abduction angle.

For both hyperextension and finger abduction the whole hand metric value is simply the sum of the single finger values. 

To get the total naive discomfort value, the 3 whole hand values were weighted with their importances and afterward added up.

\section{Random Hand Generator}

One possible weak spot, when comparing and examining hand postures is the process of choosing the hand postures to compare. In the context of this thesis, as many diverse hand postures as possible had to be compared in order to get universally valid picture. To solve this, one possible approach is to randomly generate hand postures. 
For this, a \texttt{RandomHandGenerator} was created. The class provides a function that returns a random \texttt{AngleBasedHandModel}. Internally the function again chooses from 4 different random hand generators randomly. The different functions are: \texttt{createRandomRandom(), createRandomFromSaved(), createRandomFromSavedMorph(), createRandomProcedural()}. 

The \texttt{createRandomRandom()} function was the first method to be created. It uses maximum and minimum joint angle values, defining the Range of Motion (ROM) \cite{apostolico2014postura}. For each individual finger an individual flexion and abduction within the predefined ROM is computed.

In the \texttt{createRandomFromSaved()} a hand posture is composed by randomly picking finger postures from the training data. For every finger a random TrainingUnit is picked from the PostureDataHandler and that specific finger is taken over by the new random hand. This way, many interesting hand posture combinations can be created of the 10 different training data postures.

\texttt{createRandomFromSavedMorph()} also works using the training data. For the creation of a random hand, two random \texttt{TrainingUnits} are again picked. The resulting hand is created, by interpolating the two hand postures, with the weight varying between the fingers. 

Finally the \texttt{createRandomProcedural()} had a set of predefined finger states, similar as described in the paper of Su et al. \cite{su1994logical}. For the thumb the states \texttt{Under, Downward, Aligned} and \texttt{Sideways} were defined, the remaining fingers had the possible states \texttt{HyperExtended, Flat, Fist} and \texttt{ForwardFist}. Each of the hand's fingers is assigned one state randomly and a random global abduction value is generated, defining the total fanning of the fingers, before assembling the actual hand posture from predefined finger configurations corresponding to the finger states.

To guarantee a homogenous distribution of comfort and discomfort among the randomly generated hand postures, a random range of possible naive comfort/discomfort metric values for the hand posture is defined beforehand. The random hand generation process is repeated until the calculated naive metric value of the hand posture lies within the specified range.

\section{User Study Tests}

For investigating the main questions of the thesis, a test environment had to be created. In order to verify and to improve the metric, the idea was to let participants rate different hand postures and compare the ratings with the computed metric values. Additionally, the objective was to show the impact of comfort and discomfort on precision and performance in different tasks, for that a target shooting test and a line tracing test were implemented.
The user study was designed for a seated participant in front of a desk, with the elbow of the dominant hand resting on the edge. For interaction a controller and a monitor were placed on the desk. Additionally a Leap Motion controller was attached to the desk.  

\subsection{Hand Posture Evaluation}

For the hand posture evaluation, a \texttt{UserStudyComfortEvaluation} scene was created. To begin the test, a random hand posture was generated and visualized using the \texttt{OutputHand} known from the \texttt{TrainingManager} scene. Afterwards the participant had to imitate the hand posture and rate the hand posture on an intuitive comfort/discomfort scale ranging from 0 (extremely uncomfortable) to 10 (extremely comfortable). If the participant was unable to mimic the hand posture due to its complexity, the posture was supposed to be rated with a 0. After confirming his or her choice, the results were logged to a .CSV file. For that, the naive and improved metric components were computed and logged together with the participants rating and name. In addition the whole randomly generated hand was logged, in order to be able to do further calculations later on if necessary.

Before the first hand posture evaluation the participant was shown two different randomly generated hand postures, to get a reference on what to expect in terms of comfort and discomfort. The first hand posture was supposed to be a relaxed one that could be rated with a 10, therefore with a metric value close to 0. The second hand posture should represent an extremely uncomfortable hand posture resulting in a 0 rating, therefore the metric value should be as high as possible.

\subsection{Precision and Performance Tests}

Before the target shooting and the line tracing test. The participant was again shown a randomly generated hand posture, that was meant to be tested. Again the user had to mimic the hand posture with the dominant hand and again the user had to rate the hand. After rating, the subject had to hold his hand over the Leap while holding the posture. In order to track the hand posture during the tests, the hand posture had to be learned by the k-NN. For that the Leap recorded 50 samples of the hand and the hand posture data was saved as \texttt{TrainingUnit}s by the \texttt{PostureDataHandler}. 

After that the actual tests started. If the participants broke the hand posture they were kindly asked to correct it. During both tests the participants were shown a visualization of the given hand posture in the bottom right corner as a reminder. They had to perform different tasks using their dominant hand while maintaining the hand posture. 
For that, the subjects were shown a minimalistic representation of their hand position that was recorded with the Leap. In favor of comparability, the pointing direction was universally set independent of the hand posture as the forward direction of the hand base, and was visualized with a laser beam. 

\subsection{Target Shooting Test}

The target shooting test was chosen to test the performance impact in context of quick punctual pointing. This specific test was chosen due to its similarity to real world tasks, such as giving a robot quick spatial commands or commanding units to perform some action somewhere in space in a RTS game. To implement this, the \texttt{UserStudyTargetShooting} scene was created.

The scene featured an \texttt{OutputHand} visualizing a total of 18 targets which appeared individually in a random order. The participant then had to aim down the individual targets with their hand and shoot them with the press of a button.
Once a target was hit, the distance of the hit to the target's center was computed as a metric for accuracy. The accuracy value, the time to hit as well as the different metric components and the whole hand were then logged in a .CSV file.  

When the participants broke the hand posture, shooting was locked, until the hand posture was corrected. 

\subsection{Line Tracing Test}

The line tracing test was chosen due to its similarity to real world tasks such as drawing a route to be traversed by a robot or patrolled by units in a RTS.
For this, the \texttt{UserStudyLineTracing} scene was created.

In the test, the participant was shown a blue curve on a drawing plane, that had to be traced. For that the user had to use the ray, to draw his own line on the drawing plane as close to the given curve as possible by holding a controller button. If the participant broke the hand posture, the drawing was interrupted, if the user released the drawing button, the test was ended after a certain timeout.
After the test was ended, the total accuracy was calculated and was logged together with the total task completion time, hand posture metric values and the whole hand in a .CSV file.

The curves were implemented as discrete sets of \texttt{LinePoint}s that were connected to their neighbors with line segments. In order to get a smooth curve 12 random points were generated on the drawing plane and a bezier curve was fitted through them. The bezier curve was then discretized in 75 \texttt{LinePoint}s. For drawing the line, a \texttt{LinePoint} was set at the starting point and a second was moved with the ray on the drawing plane. As soon as the second \texttt{LinePoint} moved too far away from the previous \texttt{LinePoint}, it was fixed to its position and a new \texttt{LinePoint} was added in and moved further instead.

The accuracy could be determined by calculating the average minimum distance of every drawn \texttt{LinePoint} to the given curve defined by its \texttt{LinePoint}s. The same was done the other way around and added up to the total line accuracy value. 

\subsection{The User Study Management}

In order to automate the user study process, a \texttt{UserStudyDevScreen} scene was added in. The scene showed a GUI, that can be used configure a whole user study for one participant. The experimenter can set the name of the participant, his handedness and choose a \texttt{Pose} to be overwritten by the users hand recordings. Further it can be selected which of the three user study test have to be performed and how often these will be repeated for different hand postures. 

After confirming the setting, the user study will start with the desired number of hand posture evaluations and automatically switch to the precision and performance tests. After finishing the complete user study, the participant is shown a ending screen, thanking him for participating in the study and giving him the opportunity to play around with the standard leap hand visualization. 

\section{Problems}

\subsection{Serialization and Normalization of Vector3 and Quaternions}

\subsection{Leap Tracking}

\subsection{Hand Posture Detection}

